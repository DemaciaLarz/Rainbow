{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "import import_ipynb\n",
    "from helpers import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class Train:\n",
    "    '''Runs a Rainbow experiment.\n",
    "    \n",
    "    Attributes:\n",
    "    test_results: dict of test scores.\n",
    "    Score: list of scores.\n",
    "    Loss: list of losses.\n",
    "    memory_size: int, size of memory buffer.\n",
    "    s_shape: tuple of ints, shape of input state.\n",
    "    num_a: int, number of available actions.\n",
    "    agent_names: list of string of legal agent names. Those names are:\n",
    "                : 'dqn',\n",
    "                : 'double_dqn',\n",
    "                : 'multi_step_dqn',\n",
    "                : 'per_dqn',\n",
    "                : 'dueling_dqn',\n",
    "                : 'distributional_dqn'\n",
    "    file_name: str with filename.\n",
    "    save_memory: numpy array or list, agents memory.\n",
    "    agents: class with various DQN agents and related methods.\n",
    "    q_net: Keras model object, the q value approximator.\n",
    "    t_net: Keras model object, the target value approximator.\n",
    "    memory: class, instantiates appropriate replay buffer.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                agent_name='dqn',\n",
    "                episodes=3000,\n",
    "                update_rate=500,\n",
    "                smoothing=60,\n",
    "                print_rate=3,\n",
    "                alpha=0.001,\n",
    "                decay_rate=0.9995,\n",
    "                gamma=0.99,\n",
    "                n_steps=False,\n",
    "                n_horizon=0.1,\n",
    "                trial=1,\n",
    "                batchsize=36,\n",
    "                frame_skipping=4,\n",
    "                per_alpha=0.6,\n",
    "                per_beta=0.4,\n",
    "                v_min=-10.,\n",
    "                v_max=10.,\n",
    "                num_atoms=51):\n",
    "        '''Initializes Train.\n",
    "        \n",
    "        Arguments:\n",
    "        agent_name: str with the name of the agent to train.\n",
    "        environment: str with the gym environment.\n",
    "        episodes: int, number of episodes.\n",
    "        update_rate: int, number of episodes between updates of target net.\n",
    "        smoothing: int, average plotting over this many points.\n",
    "        alpha: float, the learning rate.\n",
    "        decay_rate: float, epsilon rate of decay.\n",
    "        gamma: float, the discount rate in the TD error.\n",
    "        n_steps: int, number of n-steps in multi step agent.\n",
    "        n_horizon: float, window from which to sample n_steps from.\n",
    "        trial: int, addition to filename to mark multiple runs.\n",
    "        batchsize: int.\n",
    "        frame_skipping: int, number of frames between choosing an action.\n",
    "        per_alpha: float, alpha parameter for PER.\n",
    "        per_beta: float, beta parameter for PER.\n",
    "        v_min: float, minimum in c51 target support.\n",
    "        v_max: float, maximum in c51 target support.\n",
    "        num_atoms: int, number of atoms in c51 support.\n",
    "        \n",
    "        '''\n",
    "        self.agent_name = agent_name\n",
    "        self.episodes = episodes\n",
    "        self.update_rate = update_rate\n",
    "        self.smoothing = self.episodes // smoothing\n",
    "        self.print_rate = self.episodes // print_rate\n",
    "        self.alpha = alpha\n",
    "        self.decay_rate = decay_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.trial = trial\n",
    "        self.batchsize = batchsize\n",
    "        self.frame_skipping = frame_skipping\n",
    "        self.per_alpha = per_alpha\n",
    "        self.per_beta = per_beta\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.num_atoms = num_atoms\n",
    "        self.test_results = None\n",
    "        self.Score = None\n",
    "        self.Loss = None\n",
    "        self.memory_size = int((self.episodes * 200) * 0.5)\n",
    "        self.n_horizon = int(self.memory_size * n_horizon)\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.s_shape = self.env.observation_space.sample().shape\n",
    "        self.num_a = self.env.action_space.n\n",
    "        self.agent_names = [\n",
    "            'dqn',\n",
    "            'double_dqn',\n",
    "            'multi_step_dqn',\n",
    "            'per_dqn',\n",
    "            'dueling_dqn',\n",
    "            'distributional_dqn'\n",
    "        ]\n",
    "        assert self.agent_name in self.agent_names, 'Consider eneter a legal agent name.'\n",
    "        self.filename = None\n",
    "        self.save_memory = None\n",
    "        self.agents = DqnAgents()\n",
    "        self.q_net = None\n",
    "        self.t_net = None\n",
    "        self.memory = None\n",
    "        \n",
    "    def _get_epsilon(self, epsilon, episode):\n",
    "        '''Calculates a decaying epsilon.\n",
    "        \n",
    "        Arguments:\n",
    "        epsilon: float of current value of epsilon.\n",
    "        episode: int, current epsiode.\n",
    "        \n",
    "        Returns:\n",
    "        epsilon: float of a deacyed epsilon value.\n",
    "        \n",
    "        '''\n",
    "        if epsilon > 0.1:\n",
    "            epsilon = epsilon * self.decay_rate**episode\n",
    "        else:\n",
    "            epsilon = 0.1\n",
    "        return epsilon\n",
    "        \n",
    "    def _get_agent(self):\n",
    "        '''Instantiates models and memory according to chosen agent.'''\n",
    "        if self.agent_name in self.agent_names[:4]:\n",
    "            self.q_net = self.agents.build_dqn_graph(self.s_shape, \n",
    "                                                     self.num_a, \n",
    "                                                     self.alpha, \n",
    "                                                     compiled=True)\n",
    "            self.t_net = self.agents.build_dqn_graph(self.s_shape, \n",
    "                                                     self.num_a, \n",
    "                                                     self.alpha,\n",
    "                                                     compiled=False)\n",
    "        elif self.agent_name == 'dueling_dqn':\n",
    "            self.q_net, self.q_net_policy = self.agents.build_dueling_graph(self.s_shape,\n",
    "                                                                            self.num_a,\n",
    "                                                                            self.alpha,\n",
    "                                                                            self.batchsize)\n",
    "            self.t_net = self.agents.build_dueling_graph(self.s_shape,\n",
    "                                                         self.num_a,\n",
    "                                                         self.alpha,\n",
    "                                                         self.batchsize,\n",
    "                                                         compiled=False)\n",
    "        elif self.agent_name == 'distributional_dqn':\n",
    "            self.q_net, self.q_net_policy = self.agents.build_c51_graph(self.s_shape,\n",
    "                                                                        self.num_a,\n",
    "                                                                        self.num_atoms,\n",
    "                                                                        self.alpha,\n",
    "                                                                        self.batchsize)\n",
    "            self.q_net, self.q_net_policy = self.agents.build_c51_graph(self.s_shape,\n",
    "                                                                        self.num_a,\n",
    "                                                                        self.num_atoms,\n",
    "                                                                        self.alpha,\n",
    "                                                                        self.batchsize,\n",
    "                                                                        compiled=False)\n",
    "            \n",
    "        # experience replay for dqn, ddqn and multi step.\n",
    "        if self.agent_name in self.agent_names[:3] or self.agent_name == 'distributional_dqn':\n",
    "            self.memory = ExperienceReplay(self.memory_size,\n",
    "                                          self.batchsize)\n",
    "        else: \n",
    "            self.memory = PrioritizedExperienceReplay(self.memory_size,\n",
    "                                                     self.batchsize)\n",
    "    def _subtract_mean(self, args):\n",
    "        '''Final layer module in the dueling architecture.\n",
    "        \n",
    "        Arguments:\n",
    "        args: list of Keras tensor objects of layer outputs.\n",
    "        num_a: int with the number of available actions.\n",
    "        batchsize: int.\n",
    "        \n",
    "        Returns:\n",
    "        action values: Keras tensor object, the model output.\n",
    "        \n",
    "        '''\n",
    "        v, A = args\n",
    "        A_mean = tf.math.reduce_mean(A)\n",
    "        A_sub_mean = tf.math.subtract(A, A_mean)\n",
    "        V = tf.broadcast_to(v, [self.batchsize, self.num_a])\n",
    "        return tf.math.add(V, A_sub_mean)\n",
    "        \n",
    "    def _get_agent2(self):\n",
    "        '''Instantiates various network graphs.\n",
    "        \n",
    "        Temporary method due to some errors experienced when\n",
    "        instantiating Keras models from the DqnAgents class.\n",
    "        '''\n",
    "        if self.agent_name == 'dueling_dqn':\n",
    "            inputs = Input(shape=self.s_shape)\n",
    "            x = Dense(400, activation='relu')(inputs)\n",
    "            x = Dense(200, activation='relu')(x)\n",
    "            v = Dense(100, activation='relu')(x)\n",
    "            v = Dense(1, activation='linear')(v)\n",
    "            a = Dense(100, activation='relu')(x)\n",
    "            a = Dense(self.num_a, activation='linear')(a)\n",
    "            outputs = Lambda(self._subtract_mean)([v, a])\n",
    "            self.t_net = Model(inputs, outputs)\n",
    "            self.q_net = Model(inputs, outputs)\n",
    "            self.q_net_policy = Model(inputs, a)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=self.alpha)\n",
    "            self.q_net.compile(optimizer=opt, loss='mse')\n",
    "            #self.q_net.compile(optimizer=opt, loss=tf.keras.losses.Huber())\n",
    "            self.memory = PrioritizedExperienceReplay(self.memory_size,\n",
    "                                                     self.batchsize,\n",
    "                                                     self.per_alpha,\n",
    "                                                     self.per_beta)\n",
    "        elif self.agent_name == 'distributional_dqn':\n",
    "            inputs = Input(shape=self.s_shape)\n",
    "            x = Dense(100, activation='relu')(inputs)\n",
    "            x = Dense(100, activation='relu')(x)\n",
    "            outputs = [Dense(self.num_atoms, activation='linear')(x) for _ in range(self.num_a)]\n",
    "            self.t_net = Model(inputs, outputs)\n",
    "            self.q_net = Model(inputs, outputs)\n",
    "            opt = Adam(learning_rate=self.alpha)\n",
    "            loss = CategoricalCrossentropy(from_logits=False)\n",
    "            self.q_net.compile(optimizer=opt, loss='mse')\n",
    "            self.memory = ExperienceReplay(self.memory_size,\n",
    "                                              self.batchsize)\n",
    "        else:\n",
    "            inputs = Input(shape=self.s_shape)\n",
    "            x = Dense(400, activation='relu')(inputs)\n",
    "            x = Dense(200, activation='relu')(x)\n",
    "            outputs = Dense(self.num_a, activation='linear')(x)\n",
    "            self.t_net = Model(inputs, outputs)\n",
    "            self.q_net = Model(inputs, outputs)\n",
    "            if self.agent_name == 'per_dqn':\n",
    "                opt = tf.keras.optimizers.Adam(learning_rate=self.alpha)\n",
    "                self.q_net.compile(optimizer=opt, loss=tf.keras.losses.Huber())\n",
    "            else:\n",
    "                opt = RMSprop(learning_rate=self.alpha)\n",
    "                self.q_net.compile(optimizer=opt, loss='mse')\n",
    "            if self.agent_name == 'per_dqn':\n",
    "                self.memory = PrioritizedExperienceReplay(self.memory_size,\n",
    "                                                         self.batchsize,\n",
    "                                                         self.per_alpha,\n",
    "                                                         self.per_beta)\n",
    "            else:\n",
    "                self.memory = ExperienceReplay(self.memory_size,\n",
    "                                              self.batchsize)\n",
    "            \n",
    "    def _color_map(self):\n",
    "        '''Matplotlib color map.'''\n",
    "        Blue_1 = '#2CBDFE'\n",
    "        Green1 = '#47DBCD'\n",
    "        Pink1 = '#F3A0F2'\n",
    "        Purple1 = '#9D2EC5'\n",
    "        Violet1 = '#661D98'\n",
    "        Amber1 = '#F5B14C'\n",
    "        color_list =[Blue_1, Green1, Pink1, Purple1, Violet1, Amber1]\n",
    "        random.shuffle(color_list)\n",
    "        plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "            \n",
    "    def _train_visualizsation(self, episode, if_loss=False):\n",
    "        '''Prints and plots results from training.\n",
    "        \n",
    "        Arguments:\n",
    "        episode: int, episode.\n",
    "        \n",
    "        '''\n",
    "        score = [np.average(self.Score[i:i+self.smoothing]) for i in range(len(self.Score))]\n",
    "        loss = [np.average(self.Loss[i:i+self.smoothing]) for i in range(len(self.Loss))]\n",
    "        ############\n",
    "        var_t = [np.average(self.var_targets[i:i+self.smoothing]) for i in range(len(self.var_targets))]\n",
    "        var_p = [np.average(self.var_predictions[i:i+self.smoothing]) for i in range(len(self.var_predictions))]\n",
    "        ##########\n",
    "        self._color_map()\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(score, label='score')\n",
    "        plt.xticks([])\n",
    "        plt.xlabel('episode: ' + str(episode + 1))\n",
    "        plt.ylabel('score')\n",
    "        if if_loss:\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            plt.plot(loss, label='loss')\n",
    "            plt.xticks([])\n",
    "            plt.xlabel('episode: ' + str(episode + 1))\n",
    "            plt.ylabel('loss')\n",
    "            \n",
    "            ########\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            plt.plot(var_t, label='var targs')\n",
    "            plt.plot(var_p, label='var preds')\n",
    "            plt.xticks([])\n",
    "            plt.xlabel('episode: ' + str(episode + 1))\n",
    "            plt.ylabel('variance')\n",
    "            plt.legend(loc='upper left', frameon=False)\n",
    "            ########\n",
    "        plt.show()\n",
    "        print(f'{self.agent_name} *** avg_train_score={np.average(self.Score)}')\n",
    "        \n",
    "    def test_visualization(self, art=False):\n",
    "        '''Prints and plots tests from testing.\n",
    "        \n",
    "        Arguments:\n",
    "        art: bool, whether to show a plot or not.\n",
    "        \n",
    "        '''\n",
    "        X = []\n",
    "        for i, j in zip(self.test_results['results'], self.test_results['filenames']):\n",
    "            avg_score = np.average(i)\n",
    "            print(f'{j} *** avg_test_score={avg_score}')\n",
    "            X.append(avg_score)\n",
    "        if art:\n",
    "            self._color_map()\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            plt.bar(np.arange(len(X)), X)\n",
    "            plt.xticks(np.arange(len(X)))\n",
    "            plt.xticks(range(5), labels=self.test_results['filenames'], rotation=270)\n",
    "            plt.ylabel('average testscore')\n",
    "            plt.show()\n",
    "            \n",
    "    def get_variance(self, targets):\n",
    "        '''Computes the variance in c51 distributions.\n",
    "    \n",
    "        Arguments:\n",
    "        targets: numpy array of targets.\n",
    "\n",
    "        Returns:\n",
    "        var_target: float, mean variance from batch of targets.\n",
    "        var_pred: float, mean variance from batch of predictions.\n",
    "\n",
    "        '''\n",
    "        return np.var(targets), np.var(self.agents.z)\n",
    "            \n",
    "    def results_visualization(self):\n",
    "        '''Plots the combined results from a series of runs.'''\n",
    "\n",
    "    def test(self, load=False):\n",
    "        '''Tests a trained agent.\n",
    "        \n",
    "        Arguments:\n",
    "        load: bool, a list of str if True. Filenames to load.\n",
    "        \n",
    "        '''\n",
    "        if load:\n",
    "            models = [tensorflow.keras.models.load_model(i+'.txt') for i in load]\n",
    "        else:\n",
    "            models = [self.q_net]\n",
    "            load = [self.agent_name]\n",
    "        self.test_results = {'results': [], 'filenames': []}\n",
    "        for i, model in enumerate(models):\n",
    "            filename = load[i]\n",
    "            Score = []\n",
    "            for episode in range(100):\n",
    "                s = self.env.reset()\n",
    "                t = False\n",
    "                score = 0\n",
    "                while not t:\n",
    "                    if self.agent_name == 'dueling_dqn':\n",
    "                        a = self.agents.policy(self.q_net_policy, s, self.num_a, epsilon=0)\n",
    "                    elif self.agent_name == 'distributional_dqn':\n",
    "                        a = self.agents.policy(self.q_net, s, self.num_a, epsilon=0, c51=True)\n",
    "                    else:\n",
    "                        a = self.agents.policy(model, s, self.num_a, epsilon=0)\n",
    "                    s2, r, t, _ = self.env.step(a)\n",
    "                    score += r\n",
    "                    s = s2\n",
    "                Score.append(score)\n",
    "            with open(filename + '_test' + '.txt', 'w') as file:\n",
    "                for item in Score:\n",
    "                    file.write(str(item))\n",
    "            self.test_results['results'].append(Score)\n",
    "            self.test_results['filenames'].append(filename)\n",
    "        \n",
    "    def train(self):\n",
    "        '''Trains an agent by executing a train loop.'''\n",
    "        #self._get_agent()\n",
    "        self.Score = []\n",
    "        #### test stuff ####\n",
    "        self._get_agent2()\n",
    "        self.var_targets = []\n",
    "        self.var_predictions = []\n",
    "        ######################\n",
    "        self.filename = self.agent_name + '_' + str(self.trial)\n",
    "        self.Loss = []\n",
    "        epsilon = 1\n",
    "        delta = 1\n",
    "        for episode in range(self.episodes):\n",
    "            s = self.env.reset()\n",
    "            score = 0\n",
    "            loss = []\n",
    "            #####\n",
    "            var_targets = []\n",
    "            var_predictions = []\n",
    "            #####\n",
    "            t = False\n",
    "            epsilon = self._get_epsilon(epsilon, episode)\n",
    "            if episode % self.update_rate == 0:\n",
    "                weights = self.q_net.get_weights()\n",
    "                self.t_net.set_weights(weights)\n",
    "            for step in range(201):\n",
    "                if step % self.frame_skipping == 0: \n",
    "                    if self.agent_name == 'dueling_dqn':\n",
    "                        a = self.agents.policy(self.q_net_policy, s, self.num_a, epsilon)\n",
    "                    elif self.agent_name == 'distributional_dqn':\n",
    "                        a = self.agents.policy(self.q_net, s, self.num_a, epsilon, c51=True)\n",
    "                    else:\n",
    "                        a = self.agents.policy(self.q_net, s, self.num_a, epsilon)\n",
    "                s2, r, t, _ = self.env.step(a)\n",
    "                if self.memory.len_memory() > (0.1 * self.memory_size):\n",
    "                    if self.agent_name == 'dqn':\n",
    "                        transitions = self.memory.sample_transition()\n",
    "                        self.agents.unpack_experience(transitions, self.batchsize)\n",
    "                        targets, S = self.agents.dqn_targets(self.q_net,\n",
    "                                                             self.t_net, \n",
    "                                                             self.gamma, \n",
    "                                                             self.batchsize)\n",
    "                    elif self.agent_name == 'double_dqn':\n",
    "                        transitions = self.memory.sample_transition()\n",
    "                        self.agents.unpack_experience(transitions, self.batchsize)\n",
    "                        targets, deltas, S = self.agents.ddqn_targets(self.q_net,\n",
    "                                                                      self.t_net, \n",
    "                                                                      self.gamma, \n",
    "                                                                      self.batchsize)\n",
    "                    elif self.agent_name in ['per_dqn', 'dueling_dqn']:\n",
    "                        transitions, idxs, is_w = self.memory.sample_transition()\n",
    "                        self.agents.unpack_experience(transitions, self.batchsize)\n",
    "                        targets, deltas, S = self.agents.per_targets(self.q_net,\n",
    "                                                                     self.t_net, \n",
    "                                                                     self.gamma, \n",
    "                                                                     self.batchsize,\n",
    "                                                                     is_w)\n",
    "                    elif self.agent_name == 'multi_step_dqn':\n",
    "                        transitions = self.memory.sample_transition(n_step=(self.n_steps,\n",
    "                                                                           self.n_horizon))\n",
    "                        self.agents.unpack_experience(transitions, self.batchsize, n_step=True)\n",
    "                        targets, S = self.agents.multi_step_targets(self.q_net,\n",
    "                                                                   self.t_net,  \n",
    "                                                                   self.gamma, \n",
    "                                                                   self.batchsize,\n",
    "                                                                   self.n_steps)\n",
    "                    elif self.agent_name == 'distributional_dqn':\n",
    "                        transitions = self.memory.sample_transition()\n",
    "                        self.agents.unpack_experience(transitions, self.batchsize)\n",
    "                        targets, S = self.agents.c51_targets(self.q_net,\n",
    "                                                             self.t_net,\n",
    "                                                             self.num_a,\n",
    "                                                             self.v_min,\n",
    "                                                             self.v_max,\n",
    "                                                             self.num_atoms,\n",
    "                                                             self.gamma,\n",
    "                                                             self.batchsize)\n",
    "                    #####\n",
    "                    self.targs = targets\n",
    "                    #var_t, var_p = self.get_variance(targets)\n",
    "                    #var_targets.append(var_t)\n",
    "                    #var_predictions.append(var_p)\n",
    "                    #####    \n",
    "                    if self.agent_name in ['per_dqn', 'dueling_dqn']:\n",
    "                        loss.append(self.q_net.train_on_batch(S, targets, is_w))\n",
    "                    else:\n",
    "                        loss.append(self.q_net.train_on_batch(S, targets))\n",
    "                score += r\n",
    "                self.memory.store_transition(delta, (s, a, r, s2, t))\n",
    "                s = s2\n",
    "                if t:\n",
    "                    break\n",
    "            #####\n",
    "            #self.var_targets.append(np.average(var_targets))\n",
    "            #self.var_predictions.append(np.average(var_predictions))\n",
    "            #####\n",
    "            self.Loss.append(np.average(loss))\n",
    "            self.Score.append(score)\n",
    "            if (episode + 1) % self.print_rate == 0:\n",
    "                self._train_visualizsation(episode, if_loss=True)\n",
    "                with open(self.filename + '.txt', 'w') as file:\n",
    "                    for item in self.Score:\n",
    "                        file.write(str(item))\n",
    "        self.test()\n",
    "        self.test_visualization(art=False)\n",
    "        self.q_net.save(self.filename + '_qnet' + '.h5')\n",
    "        self.t_net.save(self.filename + '_tnet' + '.h5')\n",
    "        K.clear_session()\n",
    "    \n",
    "    def train_multiple(self, runs=[]):\n",
    "        '''Executes a series of train loops over a number of run variables.\n",
    "        \n",
    "        Arguments:\n",
    "        runs: list of indicies of agent names.\n",
    "        \n",
    "        '''\n",
    "        for run in runs:\n",
    "            self.agent_name = self.agent_names[run]\n",
    "            print('\\n','\\n', '*'*65, '\\n')\n",
    "            self.train()\n",
    "            \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    t = Train(\n",
    "        agent_name='dqn',\n",
    "        episodes=10000,\n",
    "        smoothing=200,\n",
    "        n_steps=2,\n",
    "        print_rate=20,\n",
    "        trial=5,\n",
    "        n_horizon = 0.2,\n",
    "        update_rate=15000,\n",
    "        alpha=0.00025,\n",
    "        batchsize=32,\n",
    "        per_alpha=0.6,\n",
    "        per_beta=0.5,\n",
    "        num_atoms=35,\n",
    "        v_min=-10,\n",
    "        v_max=10)\n",
    "    \n",
    "    t.train_multiple(\n",
    "        [2,])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
